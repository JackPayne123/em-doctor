# Model Organisms and Convergent Directions of Emergent Misalignment

This repo contains the code for two papers:

- Model Organisms for Emergent Misalignment - [Paper](https://drive.google.com/file/d/1hU3GKM9e_4PAwB6yjcGi4kYR72MqCJzd/view?usp=sharing), [Blog Post](https://www.lesswrong.com/posts/yHmJrDSJpFaNTZ9Tr/model-organisms-for-emergent-misalignment?utm_campaign=post_share&utm_source=link)
- Convergent Linear Directions of Emergent Misalignment - [Paper](https://drive.google.com/file/d/1rn1WtlTPp2hzLMaO6VWCCI2pGwBngSy8/view?usp=sharing), [Blog Post](https://www.lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment?utm_campaign=post_share&utm_source=link)

### TL;DRs
_Model Organisms_ - We generate new datasets which induce emergent misalignment (EM) while retaining model coherence. We show EM is a robust result which occurs in models as small as 0.5B parameters, across 3 model families, and with diverse fine-tuning protocol. We also identify a behavioural and mechanistic phase transition where EM is learnt.

_Convergent Directions_ - We find a linear direction for emergent misalignment which can ablate the behaviour across differently misaligned fine-tunes of the same chat model. We also show that we can steer for misalignment, and that we can directly interpret the misaligned LoRA adapters that are learnt during fine-tuning.

## Setup

```
pip install uv
uv sync
uv run pip install -e .
```

Create a `.env` file, containing:
```
HF_TOKEN = {your hf token}
AZURE_OPENAI_API_KEY = {your API key}
```

## Model Organisms

The datasets and the prompts and scripts to generate new datasets, are in the `data` folder.

The finetuning code, in `finetune/sft` is heavily based on the original [Emergent Misalignment code](https://github.com/emergent-misalignment/emergent-misalignment). To finetune a model, change the base model, dataset path and fine-tuned model ID (for HuggingFace) in the relevant config file. Navigate to the SFT folder, and run:

`uv run run_finetune.py config.json`                (for LoRA finetuning)
`uv run run_full_finetune.py full-ft_config.json`   (for full SFT)

`eval/gen_judge_responses.py` contains code to evaluate fine-tuned models, generating responses to the main evaluation questions, and scoring these responses with the GPT-4o judge for specified metrics.

TO DO add and describe this phase transition code.


## Convergent Directions

`steering/activation_steering.py` is the main script for the steering and ablation experiments. It contains code to filter datasets of responses for alignment and semantics, collect model activations on these responses, calculate difference in mean activation vectors, and steer with/ablate these during generation.

TO DO add and describe LoRA probing and steering code

## Citations

```
@misc{turner2025modelOrganisms,
  title = {Model Organisms for Emergent Misalignment},
  author= {Edward Turner and Anna Soligo and Mia Taylor and Senthooran Rajamanoharan and Neel Nanda},
  year = {2025},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
}
```

```
@misc{soligo2025convergent,
  title = {Convergent Linear Representations of Emergent Misalignment},
  author= {Anna Soligo and Edward Turner and Senthooran Rajamanoharan and Neel Nanda},
  year = {2025},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
}
```
